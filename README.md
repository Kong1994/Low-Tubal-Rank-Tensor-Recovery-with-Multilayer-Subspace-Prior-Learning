# Low-Tubal-Rank-Tensor-Recovery-with-Multilayer-Subspace-Prior-Learning (Pattern Recognition 2023)
code for following paper:  
 Weichao Kong, Feng Zhang, Wenjin Qin, Jianjun Wang,  "Low-TubalRank Tensor Recovery with Multilayer Subspace Prior Learning"  
[[Paper]](https://www.sciencedirect.com/science/article/pii/S0031320323002455)
### Requirements
This data of yaleB01.mat and news.mat can get from the [[NetDisk]](https://pan.baidu.com/s/1fKpHkhQKaqIuRMa-fm8CmA) (Password:m1jo) 
### Citation
If you find the code helpful in your resarch or work, please cite our papers.

</pre></div>

<div class="highlight-none"><div class="highlight"><pre>
@article{KONG2023109545,
title = {Low-Tubal-Rank tensor recovery with multilayer subspace prior learning},
journal = {Pattern Recognition},
volume = {140},
pages = {109545},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109545},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002455},
author = {Weichao Kong and Feng Zhang and Wenjin Qin and Jianjun Wang},
keywords = {Tensor robust principal component analysis, Tensor completion, Multilayer subspace prior information, ADMM, T-SVD},
abstract = {Currently, low-rank tensor recovery employing the subspace prior information is an emerging topic, which has attracted considerable attention. However, existing studies cannot flexibly and fully utilize the accessible subspace prior information, thereby leading to suboptimal restored performance. Aiming at addressing this issue, based on the tensor singular value decomposition (t-SVD), this article presents a novel strategy that integrates more than two layers of subspace knowledge about columns and rows of target tensor into one unified recovery framework. Specially, we first design a multilayer subspace prior learning scheme, and then apply it to two common low-rank tensor recovery problems, i.e., tensor completion and tensor robust component principal analysis. Crucially, we prove that our approach can achieve exact recovery of tensors under a significantly weaker incoherence assumption than the analogous conditions previously proposed. Furthermore, two efficient algorithms with convergence guarantees based on alternating direction method of multipliers (ADMM) are proposed to solve the corresponding models. The experimental results on synthetic and real tensor data show that the proposed algorithms outperform other state-of-the-art algorithms in terms of both qualitative and quantitative metrics.}
}
</pre></div>
